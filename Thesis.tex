\documentclass[a4paper, oneside]{discothesis}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{mathtools}
\usepackage{pgfplots}
\usepackage{subcaption}
\usepackage{tikz}
\usepackage{hyperref}
\usepackage{minted}
\usepackage{commath}
\usetikzlibrary{arrows.meta}
\usetikzlibrary{pgfplots.groupplots}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% DOCUMENT METADATA

\thesistype{Bachelor's Thesis} % Master's Thesis, Bachelor's Thesis, Semester Thesis, Group Project
\title{Arvy Heuristics for \\Distributed Mutual Exclusion}

\author{Silvan Mosberger}
\email{msilvan@student.ethz.ch}

\institute{Distributed Computing Group \\[2pt]
Computer Engineering and Networks Laboratory \\[2pt]
ETH Zürich}

\supervisors{Pankaj Khanchandani, András Papp\\[2pt] Prof.\ Dr.\ Roger Wattenhofer}

% Optionally, keywords and categories of the work can be shown (on the Abstract page)
\keywords{Arrow, Ivy, distributed directory, shared object}
\categories{Network algorithms}

\date{\today}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\frontmatter
\maketitle

\cleardoublepage

\begin{acknowledgements}
I thank my advisors Pankaj and András for the weekly discussions, which helped greatly in guiding the direction of this work and developing new ideas.
\end{acknowledgements}


\begin{abstract}
The abstract should be short, stating what you did and what the most important result is.
Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.
\end{abstract}

\tableofcontents

\mainmatter

\chapter{Introduction}

Often in computing a single resource is shared between multiple components, where only one of them may access it at a time. Components can request the resource after which they should receive it eventually. In a distributed setting an algorithm to solve this is called a distributed mutual exclusion algorithm or distributed directory protocol. A trivial solution is to dedicate a single node to be the center where all requests for the resource should go, with the disadvantage of a lot of traffic for many concurrent requests. Better solutions to this problem include the spanning tree based Arrow (also known as Raymond's algorithm) and Ivy protocols, introduced in \cite{Ray} and \cite{Ivy} respectively. The Arrow and Ivy protocols can be generalized to the Arvy algorithm, introduced in \cite{Arvy}, which allows for a wide range of different heuristics to be used for determining its behavior while still guaranteeing correctness. Other kinds of algorithms to solve this problem exist as well, such as ones based on message broadcasting or quorums.

In this thesis we come up with new Arvy heuristics and measure their empirical performance in a simulation of sequential requests.

\section{Previous Work}

\TODO{}

\section{Model}
\label{model}

We consider a complete graph $G=(V,E)$ with $n$ vertices and $m=\binom{n}{2}$ edges.

\paragraph{Metric Costs} Each edge has a cost/distance $c : E \rightarrow \mathbb{R}^+$ associated with it, representing the amount of time it takes a message to traverse it. Sensibly this function forms a metric space:
\begin{itemize}
\item The cost from a node to itself is zero: $\forall v:c(v, v)=0$
\item Costs between different nodes are positive: $\forall u,v : u\neq v\Rightarrow c(u,v)>0$
\item Costs are the same in both directions: $\forall u,v : c(u,v)=c(v,u)$
\item Triangle inequality: $\forall u,v,w : c(u,w)\leq c(u,v)+c(v,w)$
\end{itemize}

As a reasonable constraint, a node $v$ can only query costs from itself to other nodes, so it has access to $\{c(v, u)\;|\;u\in V\}$. In a realistic scenario these costs can be obtained and updated by doing regular pings to other nodes. In our model we don't consider changing costs however.

\paragraph{Instantaneous Computations} In addition, every node can be considered a machine with the ability to execute arbitrary effectful code such as reading/writing state or generating randomness. We model execution to be instantaneous such that no time passes for computations in nodes. Therefore traversing graph edges are the only place to spend time on.

\paragraph{Sequential Requests} A major simplification we make is that nodes can only request the token sequentially. So only after the token has reached the requesting node another node can make a request for it. Therefore requests can be represented as an series $R=\{r_1,r_2,\dots\},r_i\in V$ where $r_i$ encodes the $i$-th request originating from node $r_i$. While this series could be endless, for being able to reason more easily about execution time we let it be finite.

\paragraph{Request Paths} In the type of algorithms we'll look at, when the $i$-th request gets made by node $r_i$, the request travels along some path $A^{(i)}=\{a^{(i)}_0,a^{(i)}_1,\dots,a^{(i)}_{l_i}\}$ with $l_i=|A^{(i)}|-1$ being the index of the last node and also the number of messages that are being sent. Then $a^{(i)}_0=r_i$ is the node making the request and the $a^{(i)}_{l_i}$ is the node currently holding the token. After the request was sent along this path, the token is then sent back directly from $a^{(i)}_{l_i}$ to $a^{(i)}_0$ to satisfy the request. If $|A^{(i)}|=1$ the node making the request has the token already. It holds that $a^{(i)}_0=a^{(i+1)}_{l_{i+1}}$ for all $i\in\mathbb{N}$, meaning the node making a request in a request will be the last node in the request path in the following request.

\paragraph{Performance}\label{model-perf} In order to be able to talk about performance of such algorithms, we first define $c(A^{(i)})$ to be the cost of traversing request path $A^{(i)}$:
\begin{equation}
c(A^{(i)})\coloneqq\sum_{k=1}^{l_i}c(a^{(i)}_{k-1}, a^{(i)}_k)
\end{equation}

And let $c_{avg}$ be the average cost of an edge in the graph:
\begin{equation}
c_{avg}=\frac{1}{|V|^2}\sum_{u,v\in V}c(u,v)
\end{equation}

Now there are multiple reasonable ways to define performance for a sequence of requests.
\begin{itemize}
\item
  Arguably the most important performance measure is the time it takes to satisfy the requests. For a single requests this includes the cost to traverse the request path as well as the cost to send the token back to the requesting node. However since the cost to send the token back does not change between different algorithms, we ignore it to get a simpler measure for comparison. In addition to normalize the costs, we divide by $c_{avg}$. To get a result over all requests $R$ we take the average over all individual requests.
  \begin{equation}
    \mathcal{C}_{time}(R) = \frac{1}{|R|}\sum_{i=1}^{|R|}\frac{c(A^{(i)})}{c_{avg}}
  \end{equation}

\item
  A different way is to look at how much worse the request paths are in comparison to a direct connection between the first and the last node in it. We again average this over all requests $R$. If after these requests the system were to be in the same state as initially, this would resemble the competitive ratio. \TODO{Look up proper definition of comp ratio in some paper}. To avoid dividing by $0$, we only look at the requests where the node making the request isn't the one holding the token: $R'=\{r_i\in R,|A^{(i)}|\geq 2\}$. This measure penalizes more for request paths that are much longer than the direct path, so it's better suited for getting a sense of how individual requests are doing.
  \begin{equation}
    \mathcal{C}_{ratio}(R) = \frac{1}{|R'|}\sum_{i=1}^{|R'|}\frac{c(A^{(i)})}{c(a^{(i)}_0,a^{(i)}_{l_i})}
  \end{equation}

\item
  Another interesting measure is the average number of messages being sent per request. While this isn't very useful with the restrictions of instantaneous execution and sequential requests in our model, it can give us a sense of how well algorithms would do without these.
  \begin{equation}
    \mathcal{C}_{hops}(r) = \frac{1}{|R|}\sum_il_i
  \end{equation}
\end{itemize}

\section{Arrow, Ivy and Arvy}

Two well-known algorithms for solving this problem are Arrow and Ivy, which are both a special case of an Arvy algorithm, which is where its name comes from.

All of these algorithms are based on the idea of maintaining a rooted spanning tree over time: Every node stores a pointer $parent : V \rightarrow V$ pointing to its parent in the tree, while the root node points to itself. When a (non-root) node $a_{0}$ needs the token, it sends a request message towards its parent $a_{1}=parent(a_{0})$. When a node $a_{i}$ receives such a request, it forwards it to $a_{i+1}=parent(a_{i})$ and so on until the root $a_{l}$ containing the token is reached. This forms a request path $A=\{a_{0},a_{1},\dots,a_{l}\}$. The final node then finishes its own work with the token after which it sends it back directly to $a_{0}$. Now to make this a functioning algorithm, the arrows along this path need to be inverted in some way such that the rooted tree is restored.

\tikzset{
  arvy-expl/.style =
    { v/.style = { circle, draw } % normal graph vertices
    , r/.style = { v, thick, gr } % root nodes
    , rd/.style = { red!70!black }
    , bl/.style = { blue!70!black }
    , gr/.style = { green!50!black }
    , q/.style = { v, rd } % currently making request
    , e/.style = { draw=black, -> }
    , re/.style = { e, dashed, bl }
    , >={Stealth[scale=1.5]}
    , scale = 0.6
    , auto
    }
}

\subsection{Arrow}
\label{intro:arrow}

The Arrow algorithm is the simplest way to maintain a rooted spanning tree, in that it just inverts the pointers along its path: When $a_{i+1}$ receives a request from $a_{i}$, it sets $parent(a_{i+1})=a_{i}$. See Figure~\ref{fig:arrow} for a walk-through of a request with Arrow. Notable with Arrow is that it doesn't ever change the structure of the spanning tree, meaning that the initial tree is crucial for its performance.

\begin{figure}[H]
\begin{subfigure}[t]{0.5\textwidth}
\centering
\begin{tikzpicture}[arvy-expl]
\node[v] (1) at (0,5) {1};
\node[q] (2) at (7,5) {2};
\node[rd, above=0 of 2] {\scriptsize{needs token}};
\node[v] (3) at (3,4) {3};
\node[r] (4) at (2,1) {4};
\node[gr, left=0 of 4] {\scriptsize{has token}};
\node[v] (5) at (6,0) {5};
\draw[e] (2) -- (3);
\draw[e] (3) -- (4);
\draw[e] (5) -- (4);
\draw[e] (1) -- (3);
\path (4) edge [loop below] (4);
\end{tikzpicture}
\caption{Node 2 needs the token which is currently at node 4}
\end{subfigure}
\quad
\begin{subfigure}[t]{0.5\textwidth}
\centering
\begin{tikzpicture}[arvy-expl]
\node[v] (1) at (0,5) {1};
\node[q] (2) at (7,5) {2};
\node[v] (3) at (3,4) {3};
\node[r] (4) at (2,1) {4};
\node[v] (5) at (6,0) {5};
\draw[re] (2) -- node[below right=3pt and -20pt]{\scriptsize{request from 2}} (3);
\draw[e] (3) -- (4);
\draw[e] (5) -- (4);
\draw[e] (1) -- (3);
\path (4) edge [loop below] (4);
\path (2) edge [loop above] (2);
\end{tikzpicture}
\caption{Node 2 sends a request towards its parent node 3 after which it sets its parent to itself}
\end{subfigure}

\begin{subfigure}[t]{0.5\textwidth}
\centering
\begin{tikzpicture}[arvy-expl]
\node[v] (1) at (0,5) {1};
\node[q] (2) at (7,5) {2};
\node[v] (3) at (3,4) {3};
\node[r] (4) at (2,1) {4};
\node[v] (5) at (6,0) {5};
\draw[e] (3) -- (2);
\draw[re] (3) -- (4);
\draw[e] (5) -- (4);
\draw[e] (1) -- (3);
\path (4) edge [loop below] (4);
\path (2) edge [loop above] (2);
\end{tikzpicture}
\caption{Node 3 receives the request and forwards it to its parent node 4 while changing its parent to node 2 from which it received the request from}
\end{subfigure}
\quad
\begin{subfigure}[t]{0.5\textwidth}
\centering
\begin{tikzpicture}[arvy-expl]
\node[v] (1) at (0,5) {1};
\node[r] (2) at (7,5) {2};
\node[v] (3) at (3,4) {3};
\node[v] (4) at (2,1) {4};
\node[v] (5) at (6,0) {5};
\draw[e] (3) -- (2);
\draw[e] (4) -- (3);
\draw[e] (5) -- (4);
\draw[e] (1) -- (3);
\path (2) edge [loop above] (2);
\end{tikzpicture}
\caption{Node 4 receives the request, making it send the token to the requesting node. Meanwhile it updates its parent to the node it received the request from. The spanning tree is restored}
\end{subfigure}
\caption{Arrow example}
\label{fig:arrow}
\end{figure}

\subsection{Ivy}
\label{intro:ivy}

Ivy encompasses an entirely different way to invert the arrows along the request path, namely that every node $a_{i+1}$ receiving the request sets its new parent to the node that made the original request: $parent(a_{i+1})=a_{0}$. Therefore $a_{0}$ ends up being the center of a star consisting of all nodes along the request path. Figure~\ref{fig:ivy} shows an example of Ivy.

\begin{figure}[H]
\begin{subfigure}[t]{0.5\textwidth}
\centering
\begin{tikzpicture}[arvy-expl]
\node[v] (1) at (0,5) {1};
\node[q] (2) at (7,5) {2};
\node[rd, above=0 of 2] {\scriptsize{needs token}};
\node[v] (3) at (3,4) {3};
\node[v] (4) at (2,1) {4};
\node[r] (5) at (6,0) {5};
\node[gr, left=0 of 5] {\scriptsize{has token}};
\draw[re] (2) -- node[below right=3pt and -20pt]{\scriptsize{request from 2}} (3);
\draw[e] (3) -- (4);
\draw[e] (4) -- (5);
\draw[e] (1) -- (4);
\path (5) edge [loop above] (5);
\path (2) edge [loop right] (2);
\end{tikzpicture}
\caption{Node 2 needs the token which node 5 currently has. So it sends a request message towards its parent after which its sets the parent to itself}
\end{subfigure}
\quad
\begin{subfigure}[t]{0.5\textwidth}
\centering
\begin{tikzpicture}[arvy-expl]
\node[v] (1) at (0,5) {1};
\node[q] (2) at (7,5) {2};
\node[v] (3) at (3,4) {3};
\node[v] (4) at (2,1) {4};
\node[r] (5) at (6,0) {5};
\draw[e] (3) -- (2);
\draw[re] (3) -- (4);
\draw[e] (4) -- (5);
\draw[e] (1) -- (4);
\path (5) edge [loop above] (5);
\path (2) edge [loop right] (2);
\end{tikzpicture}
\caption{Node 3 forwards the request to node 4 while setting its parent to node 2 as the original request sender}
\end{subfigure}

\begin{subfigure}[t]{0.5\textwidth}
\centering
\begin{tikzpicture}[arvy-expl]
\node[v] (1) at (0,5) {1};
\node[q] (2) at (7,5) {2};
\node[v] (3) at (3,4) {3};
\node[v] (4) at (2,1) {4};
\node[r] (5) at (6,0) {5};
\draw[e] (3) -- (2);
\draw[e] (4) -- (2);
\draw[re] (4) -- (5);
\draw[e] (1) -- (4);
\path (5) edge [loop above] (5);
\path (2) edge [loop right] (2);
\end{tikzpicture}
\caption{Node 4 receives the request and forwards it further, while setting its parent to node 2 as well because that's where the request came from}
\end{subfigure}
\quad
\begin{subfigure}[t]{0.5\textwidth}
\centering
\begin{tikzpicture}[arvy-expl]
\node[v] (1) at (0,5) {1};
\node[r] (2) at (7,5) {2};
\node[v] (3) at (3,4) {3};
\node[v] (4) at (2,1) {4};
\node[v] (5) at (6,0) {5};
\draw[e] (3) -- (2);
\draw[e] (4) -- (2);
\draw[e] (5) -- (2);
\draw[e] (1) -- (4);
\path (2) edge [loop right] (2);
\end{tikzpicture}
\caption{The request arrives at node 5 holding the token, so it sends the token to the requesting node 2 while setting its new parent to the requesting node too}
\end{subfigure}
\caption{Ivy example}
\label{fig:ivy}
\end{figure}

\subsection{Arvy}

Arvy is a class of algorithms generalizing Arrow and Ivy by allowing nodes $a_{i+1}$ that received a request to set their parent to any node the request already traveled through, so $parent(a_{i+1})\in\{a_{0},\dots,a_{i}\}$. If the last possible parent is chosen this is equivalent to Arrow, while if the earliest possible parent is chosen this is equivalent to Ivy. See Figure~\ref{fig:arvy} for an example where each node chooses a random new parent out of the available ones.

\begin{figure}[]
\begin{subfigure}[t]{0.5\textwidth}
\centering
\begin{tikzpicture}[arvy-expl]
\node[v] (1) at (0,5) {1};
\draw[e] (1) -- (4);
\node[q] (2) at (7,5) {2};
\node[rd, above=0 of 2] {\scriptsize{needs token}};
\node[v] (3) at (3,4) {3};
\node[v] (4) at (2,1) {4};
\node[r] (5) at (6,0) {5};
\node[gr, left=0 of 5] {\scriptsize{has token}};
\draw[re] (2) -- node[below right=3pt and -20pt]{\scriptsize{request from 2}} (3);
\draw[e] (3) -- (4);
\draw[e] (4) -- (5);
\path (5) edge [loop above] (5);
\path (2) edge [loop right] (2);
\end{tikzpicture}
\caption{Node 2 needs the token so it sends a request for it towards its parent node 3}
\end{subfigure}
\quad
\begin{subfigure}[t]{0.5\textwidth}
\centering
\begin{tikzpicture}[arvy-expl]
\node[v] (1) at (0,5) {1};
\draw[e] (1) -- (4);
\node[q] (2) at (7,5) {2};
\node[v] (3) at (3,4) {3};
\node[v] (4) at (2,1) {4};
\node[r] (5) at (6,0) {5};
\draw[e] (3) -- (2);
\draw[re] (3) -- (4);
\draw[e] (4) -- (5);
\path (5) edge [loop above] (5);
\path (2) edge [loop right] (2);
\end{tikzpicture}
\caption{Node 3 receives the request, forwards it and chooses the only possible new parent node 2}
\end{subfigure}

\begin{subfigure}[t]{0.5\textwidth}
\centering
\begin{tikzpicture}[arvy-expl]
\node[v] (1) at (0,5) {1};
\draw[e] (1) -- (4);
\node[q] (2) at (7,5) {2};
\node[v] (3) at (3,4) {3};
\node[v] (4) at (2,1) {4};
\node[r] (5) at (6,0) {5};
\draw[e] (3) -- (2);
\draw[e] (4) -- (2);
\draw[re] (4) -- (5);
\path (5) edge [loop above] (5);
\path (2) edge [loop right] (2);
\end{tikzpicture}
\caption{Node 4 receives the request and forwards it. It can now choose to connect back to either node 2 or 3, since the request has traveled through both of them. It chooses 2}
\end{subfigure}
\quad
\begin{subfigure}[t]{0.5\textwidth}
\centering
\begin{tikzpicture}[arvy-expl]
\node[v] (1) at (0,5) {1};
\draw[e] (1) -- (4);
\node[r] (2) at (7,5) {2};
\node[v] (3) at (3,4) {3};
\node[v] (4) at (2,1) {4};
\node[v] (5) at (6,0) {5};
\draw[e] (3) -- (2);
\draw[e] (4) -- (2);
\draw[e] (5) -- (3);
\path (2) edge [loop right] (2);
\end{tikzpicture}
\caption{Node 5 receives the request for the token, making it send it to the requesting node 2. For selecting its new parent, it can choose between 2, 3 and 4, since all of those nodes lie on the request path}
\end{subfigure}

\begin{subfigure}[t]{0.5\textwidth}
\centering
\begin{tikzpicture}[arvy-expl]
\node[q] (1) at (0,5) {1};
\path (1) edge [loop right] (1);
\draw[re] (1) .. controls (4) .. node[above=2pt]{} (2);
\node[r] (2) at (7,5) {2};
\node[v] (3) at (3,4) {3};
\node[v] (4) at (2,1) {4};
\node[v] (5) at (6,0) {5};
\draw[e] (3) -- (2);
\draw[e] (5) -- (3);
\path (2) edge [loop right] (2);
\end{tikzpicture}
\caption{In a new request node 2 now wants the token, so it send a request to node 4 which forwards it to node 2. The request doesn't need to travel through node 3 because in the previous request node 4 chose 2 as its parent instead of 3}
\end{subfigure}
\quad
\begin{subfigure}[t]{0.5\textwidth}
\centering
\begin{tikzpicture}[arvy-expl]
\node[r] (1) at (0,5) {1};
\path (1) edge [loop right] (1);
\node[v] (2) at (7,5) {2};
\node[v] (3) at (3,4) {3};
\node[v] (4) at (2,1) {4};
\node[v] (5) at (6,0) {5};
\draw[e] (3) -- (2);
\draw[e] (5) -- (3);
\draw[e] (4) -- (1);
\draw[e] (2) -- (4);
\end{tikzpicture}
\caption{For choosing the new parents, node 4 can only choose 1, whereas node 2 can choose either 1 or 4 of which it chooses 4, therefore not changing the tree structure}
\end{subfigure}
\caption{Arvy example}
\label{fig:arvy}
\end{figure}


\chapter{Algorithms}

This section describes all the Arvy algorithms to be tested. A simplified version of a general algorithm can be written as follows, which we'll be using in the following sections. Note that the request path is denoted as $\{a_{i}\}$ as in previous sections.

\begin{algorithm}
\caption{Arvy algorithm}
\label{arvyalg}
\begin{algorithmic}
\Function{RequestToken}{$a_{0}$}
\Comment Node $a_{0}$ wants the token
\If{$parent(a_{0})\neq a_{0}$}
    \State send request for token to $parent(a_{0})$
    \State $parent(a_{0})\gets a_{0}$
\EndIf
\EndFunction
\Function{ReceiveRequest}{$a_{k}$}
\Comment Node $a_{k}$ receives a request for the token
\If{$parent(a_{k})=a_{k}$}
    \State send token to $a_{0}$
\Else
    \State forward request to $parent(a_{k})$
\EndIf
\State $parent(a_{k})\gets\;$\Call{SelectNewParent}{$a_{0}, a_{1}, \dots, a_{k-1}$}
\EndFunction
\Function{SelectNewParent}{$\{a_{0}, a_{1},\dots,a_{k-1}\}$}
\State\Return any of $a_{i}$
\EndFunction
\end{algorithmic}
\end{algorithm}

Any concrete algorithm will have to define an implementation of \textsc{SelectNewParent}.

\section{Arrow}

As already explained in Section~\ref{intro:arrow}, Arrow maintains the structure of the spanning tree by always selecting the most recent node as a new parent, therefore inverting the arrows.
\begin{algorithmic}
\Function{SelectNewParent}{$\{a_0, a_1,\dots,a_{k-1}\}$}
\State\Return $a_{k-1}$
\EndFunction
\end{algorithmic}

\section{Ivy}

Also explained already in Section~\ref{intro:ivy}, Ivy always connects every node along a request path to the node that made the original request
\begin{algorithmic}
\Function{SelectNewParent}{$\{a_0, a_1,\dots,a_{k-1}\}$}
\State\Return $a_0$
\EndFunction
\end{algorithmic}


\section{Uniformly Random}

Not a particularly interesting algorithm, but good as a reference point is a completely random algorithm, selecting a new parent uniformly random from all available choices.

\begin{algorithmic}
\Function{SelectNewParent}{$\{a_0, a_1,\dots,a_{k-1}\}$}
\State\Return $a_i$ uniformly at random
\EndFunction
\end{algorithmic}

\section{Edge Cost Minimizer}
\label{alg:ecm}

Since algorithms have access to the edge costs, we should make use of them. A simple algorithm is one that always chooses the node with minimum edge cost as the new parent. In case there are multiple edges with the same cost, choose the last one of them. See Figure~\ref{fig:ecm} for an example. Intuitively this heuristic should perform well since short paths make for short request paths.
\begin{algorithmic}
\Function{SelectNewParent}{$\{a_{0}, a_{1},\dots,a_{k-1}\}$}
\State\Return$a_{i}$ such that $c(a_{k},a_{i}),i\in\{0,\dots,k-1\}$ is minimal, use highest $i$ for tie-breaking
\EndFunction
\end{algorithmic}

Notable properties of this algorithm include
\begin{itemize}
\item The total edge distance in the tree can only get smaller over time, since a previous edge between $a_{k-1}$ and $a_{k}$ can only be replaced with a shorter one, which will be the same edge when $c(a_{k},a_{k-1})$ is the minimum already.
\item This in turn means that with a good enough distribution of nodes initiating requests, the tree will eventually converge to a Minimum Spanning Tree (MST), because that's the state of lowest possible total edge distance.
\item Consequently, once the tree has converged, this algorithm behaves exactly the same as Arrow, which never changes the tree by design. Therefore to get the eventual behavior of this algorithm, one can simply start with an MST directly and run Arrow on it. This seems to imply that in general this Algorithm isn't any more powerful than Arrow.
\end{itemize}

\begin{figure}
\begin{subfigure}[t]{0.5\textwidth}
\centering
\begin{tikzpicture}
[bn/.style={circle,draw}
,root/.style={bn,thick}
,be/.style={dashed,draw=blue,arrows={-Stealth[scale=1.5]}}
,req/.style={bn,red!70!black}
,auto,scale=0.7]
\node[bn] (n1) at (0,0) {$a_{0}$};
\node[bn] (n2) at (1,2) {$a_{1}$};
\node[bn] (n3) at (3,3) {$a_{2}$};
\node[bn] (n4) at (5,2) {$a_{3}$};
\node[bn] (n5) at (4,0) {$a_{4}$};
\draw[be] (n1) -- node[blue]{\scriptsize{request path}} (n2);
\draw[be] (n2) -- (n3);
\draw[be] (n3) -- (n4);
\draw[be] (n4) -- (n5);
\draw[dotted] (n1) -- node{$3$} (n5);
\draw[dotted] (n2) -- node{$5$} (n5);
\draw[dotted] (n3) -- node{$1$} (n5);
\draw[dotted] (n4) -- node{$4$} (n5);
\end{tikzpicture}
\caption{The request arrives at $a_{4}$ which now needs to decide which one of $a_{0},\dots,a_{3}$ to choose as a parent. For this it can access the edge costs to all of them.}
\end{subfigure}
\quad
\begin{subfigure}[t]{0.5\textwidth}
\centering
\begin{tikzpicture}
[bn/.style={circle,draw}
,root/.style={bn,thick}
,be/.style={dashed,draw=blue,arrows={-Stealth[scale=1.5]}}
,req/.style={bn,red!70!black}
,auto,scale=0.7]
\node[bn] (n1) at (0,0) {$a_{0}$};
\node[bn] (n2) at (1,2) {$a_{1}$};
\node[bn] (n3) at (3,3) {$a_{2}$};
\node[bn] (n4) at (5,2) {$a_{3}$};
\node[bn] (n5) at (4,0) {$a_{4}$};
\draw[be] (n1) -- (n2);
\draw[be] (n2) -- (n3);
\draw[be] (n3) -- (n4);
\draw[be] (n4) -- (n5);
\draw[thick,arrows={-Stealth[scale=1.5]}] (n5) -- node{$1$} (n3);
\end{tikzpicture}
\caption{$a_{2}$ gets chosen as the new parent since the edge towards it has the smallest cost.}
\end{subfigure}
\caption{Edge Cost Minimizer example with four node in the request path.}
\label{fig:ecm}
\end{figure}

\section{Local Pair Distance Minimizer}

An even better measure to minimize is the total distance between all pairs of nodes in the spanning tree $T$, defined as
\begin{equation}
\mathcal{C}_{pairs}(T) = \sum_{u,v\in V, u\neq v}c_T(u,v)
\end{equation}

where $c_T(u,v)$ is the cost of the shortest path between nodes $u$ and $v$ in the tree. Tree mutations aside, this directly corresponds to the performance of Arvy algorithms with uniformly random requests, since all pairs of nodes have the same probability of being used, meaning their distance in the tree contributes the same amount to the cost of an average request. In Section~\ref{tree:mpd} we discuss such minimal trees further. Notably the complexity to calculate this optimal tree is $\mathcal{O}(n^n)$, unrealistic for many nodes. In addition, nodes don't have access to a global view of the graph at runtime which is needed to calculate the minimum pair distance tree.

We can however devise an algorithm that minimizes pair distance in a local subgraph consisting of only the nodes in the request path. Specifically when node $a_{k}$ needs to select a new parent node out of the candidates $A=\{a_{0},\dots,a_{k-1}\}$, it looks at the tree graph $T=(V',E')$ (with $V'\in A$ and $E'\subset V'\times V'$) of nodes in the request path that was constructed from the parent selections of all previous nodes in the path. It then selects the node $a_{i}$ such that selecting this node yields the lowest possible pair distance, meaning $\mathcal{C}_{pairs}(T_i)$ with $T_i=(V'\cup a_{k},E'\cup(a_{i},a_{k}))$ is minimal. See Figure~\ref{fig:lpm} for an example.

\begin{algorithmic}
\Function{SelectNewParent}{$\{a_{0}, a_{1},\dots,a_{k-1}\}$}
\State\Return$a_{i}$ such that $\mathcal{C}_{pairs}(T_i)$ is minimal, use highest $i$ for tie-breaking
\EndFunction
\end{algorithmic}

\begin{figure}[H]
\begin{subfigure}[t]{0.5\textwidth}
\centering
\begin{tikzpicture}[arvy-expl,scale=2,font=\scriptsize]
\node[v] (0) at (0,0) {$a_{0}$};
\node[v] (1) at (0,2) {$a_{1}$};
\node[v] (2) at (2,3) {$a_{2}$};
\draw[e] (1) --node[left]{4} (0);
\draw[dashed] (2) --node[below=2pt]{6} (0);
\draw[e] (2) --node[above]{5} (1);
\end{tikzpicture}
\caption{$a_{2}$ chooses $a_{1}$ as its new parent, since the total pair distance for that tree $\mathcal{C}_{pairs}(T_1)=c_{T_1}(a_{0},a_{1})+c_{T_1}(a_{0},a_{2})+c_{T_1}(a_{1},a_{2})=4+9+5=18$ is smaller than the one for $a_{0}$ which would be $4+10+6=20$}
\end{subfigure}
\quad
\begin{subfigure}[t]{0.5\textwidth}
\centering
\begin{tikzpicture}[arvy-expl,scale=2,font=\scriptsize]
\node[v] (0) at (0,0) {$a_{0}$};
\node[v] (1) at (0,2) {$a_{1}$};
\node[v] (2) at (2,3) {$a_{2}$};
\node[v] (3) at (3,1) {$a_{3}$};
\draw[e] (1) --node[left]{4} (0);
\draw[dashed] (2) --node[below=2pt]{6} (0);
\draw[e] (2) --node[above]{5} (1);
\draw[dashed] (0) --node[below]{2} (3);
\draw[e] (3) --node[above]{3} (1);
\draw[dashed] (2) --node[right]{2} (3);
\end{tikzpicture}
\caption{Even though edge $(a_{3},a_{1})$ is the longest one to select, $a_{3}$ chooses $a_{1}$ as its new parent, since it minimizes the total pair distance. Here $\mathcal{C}_{pairs}(T_0)=37$, $\mathcal{C}_{pairs}(T_1)=36$, $\mathcal{C}_{pairs}(T_2)=38$}
\end{subfigure}
\caption{Local Pair Distance Minimizer example}
\label{fig:lpm}
\end{figure}

\section{Recursive Clique}

As we'll see in Section~\ref{result:clique}, Ivy performs well on cliques of less than 5 nodes. Taking advantage of this fact, we design a graph resembling a clique, but where each node can contain another clique within, and so on. See Figure~\ref{fig:reclique} for an example with 2 levels of cliques and each level having a clique of 3 nodes. This resulting recursive clique graph allows running Ivy recursively, taking advantage of Ivy's performance in each recursion level.

A recursive clique graph has a level count parameter $l$ representing the number of levels and a base parameter $b$ for the number of nodes in a clique on a single level. We call the $b$ nodes in all levels except the lowest one \textit{virtual}, because they only exist to group non-virtual nodes in lower levels together. The total number of nodes in such a graph is $b^l$. In addition, for defining costs between edges, we declare a parameter $f>1$ for the increase factor in edge distance between one level. This factor represents how far apart nodes in higher levels are. We define nodes in a clique in the lowest level to have distance $1$ from each other. For arbitrary nodes we can define the cost as

\begin{equation}
c(u,v)=\begin{cases}
0 & \text{if $u=v$} \\
1 & \text{otherwise if $u$ and $v$ share the same lowest level} \\
f & \text{otherwise if $u$ and $v$ share the same second-lowest level} \\
f^2 & \text{otherwise if $u$ and $v$ share the same third-lowest level} \\
\vdots & \\
f^{l-1} & \text{otherwise, $u$ and $v$ are in different nodes of the highest level} \\
\end{cases}
\end{equation}

When running a recursive Ivy algorithm on such a graph, the guiding rules are:
\begin{enumerate}
\item The spanning tree can only include at most one connection between all virtual nodes on a single level. This ensures that if a request was made from within the virtual node where the token currently is at, the request path does not exit that virtual node. In addition this means there a request path can include at most $b-1$ edges of length $f^{l-1}$ from the most upper level (and similarly for lower levels), therefore keeping the number of long edges at a minimum.
\label{reclique-rule-conn}
\item Connect to the earliest possible node in the request path. This ensures that the inverted arrows on the request path lead future requests to the root on the fastest way possible while still satisfying rule \ref{reclique-rule-conn}. This gives Ivy-like behavior on each level and across levels.
\label{reclique-rule-early}
\end{enumerate}

See Figure~\ref{fig:reclique-alg} for an example of how this algorithm behaves with these rules.

While in this algorithm we only limit ourselves to these three parameters for simplicity, this idea can be extended much further: Each level can contain arbitrary graphs without regularity. As an example, with three nodes at the top-most level, the first node could contain a subgraph of a star, the second node a ring graph and the third node another graph containing more subgraphs for each node. This then also calls for running different Arvy algorithms on each level, using whichever works best for that levels graph, making this a very flexible approach. In a realistic scenario, the top-most level can represent different data centers around the world, the level below different racks in the data center and the level below that different machines in a rack.

\begin{figure}
\centering
\begin{tikzpicture}
\coordinate (A) at (0:2);
\path (A) +(0:0.3) coordinate (AA);
\path (A) +(120:0.3) coordinate (AB);
\path (A) +(240:0.3) coordinate (AC);
\coordinate (B) at (120:2);
\path (B) +(0:0.3) coordinate (BA);
\path (B) +(120:0.3) coordinate (BB);
\path (B) +(240:0.3) coordinate (BC);
\coordinate (C) at (240:2);
\path (C) +(0:0.3) coordinate (CA);
\path (C) +(120:0.3) coordinate (CB);
\path (C) +(240:0.3) coordinate (CC);

\draw (A) -- node[above,sloped]{$f$} (B) -- node[left]{$f$} (C) -- node[below,sloped]{$f$} (A);

\fill[white] (A) circle [radius=17pt];
\fill[white] (B) circle [radius=17pt];
\fill[white] (C) circle [radius=17pt];
\draw (A) circle [radius=17pt];
\draw (B) circle [radius=17pt];
\draw (C) circle [radius=17pt];

\foreach \x in {A,B,C}
  \foreach \y in {A,B,C}
    \fill (\x\y) circle [radius=2pt];

\draw (AA) -- node[above,sloped]{\scriptsize{1}} (AB) -- node[left]{\scriptsize{1}} (AC) -- node[below,sloped]{\scriptsize{1}} (AA);
\draw (BA) -- node[above,sloped]{\scriptsize{1}} (BB) -- node[left]{\scriptsize{1}} (BC) -- node[below,sloped]{\scriptsize{1}} (BA);
\draw (CA) -- node[above,sloped]{\scriptsize{1}} (CB) -- node[left]{\scriptsize{1}} (CC) -- node[below,sloped]{\scriptsize{1}} (CA);

\end{tikzpicture}
\caption{Recursive Clique graph with 2 levels and 3 nodes in each level. Nodes in the same lowest level clique have distance $1$ between them, while nodes in different lowest level cliques have distance $f$ between them. Three virtual nodes group together three nodes each}
\label{fig:reclique}
\end{figure}


\begin{figure}
\begin{subfigure}[t]{0.5\textwidth}
\centering
\begin{tikzpicture}
[be/.style={draw=black,arrows={-Stealth[scale=1.5]}}]
\coordinate (A) at (0:1.8);
\path (A) +(0:0.5) coordinate (AA);
\path (A) +(120:0.5) coordinate (AB);
\path (A) +(240:0.5) coordinate (AC);
\coordinate (B) at (120:1.8);
\path (B) +(0:0.5) coordinate (BA);
\path (B) +(120:0.5) coordinate (BB);
\path (B) +(240:0.5) coordinate (BC);
\coordinate (C) at (240:1.8);
\path (C) +(0:0.5) coordinate (CA);
\path (C) +(120:0.5) coordinate (CB);
\path (C) +(240:0.5) coordinate (CC);

\draw[be] (AB) -- (AA);
\draw[be] (AC) -- (AA);
\draw[be] (BB) -- (BA);
\draw[be] (BC) -- (BA);
\draw[be] (CB) -- (CA);
\draw[be] (CC) -- (CA);
\draw[be] (BA) to[out=0,in=90] (AA);
\draw[be] (CA) to[out=0,in=-90] (AA);

\foreach \x in {A,B,C}
  \foreach \y in {A,B,C}
    \fill (\x\y) circle [radius=2pt];

\fill[green!70!black] (AA) circle [radius=2pt,green];

\fill[red] (CC) circle [radius=2pt,fill=red];
\node[below=0 of CC] {\scriptsize{$a_{0}$}};
\node[below right=0 and -5pt of CA] {\scriptsize{$a_{1}$}};
\node[right=0 of AA] {\scriptsize{$a_{2}$}};

\end{tikzpicture}
\caption{The first node $a_{0}$ on the request path makes a request for the token which lies at root node $a_{2}$. $a_{2}$ will invert its arrow to point to $a_{0}$ with a long edge, which rule~\ref{reclique-rule-conn} allows because the previous long edge $(a_{1},a_{2})$ is discarded when the request traverses it.}
\label{fig:reclique-alg-a}
\end{subfigure}
\quad
\begin{subfigure}[t]{0.5\textwidth}
\centering
\begin{tikzpicture}
[be/.style={draw=black,arrows={-Stealth[scale=1.5]}}]
\coordinate (A) at (0:1.8);
\path (A) +(0:0.5) coordinate (AA);
\path (A) +(120:0.5) coordinate (AB);
\path (A) +(240:0.5) coordinate (AC);
\coordinate (B) at (120:1.8);
\path (B) +(0:0.5) coordinate (BA);
\path (B) +(120:0.5) coordinate (BB);
\path (B) +(240:0.5) coordinate (BC);
\coordinate (C) at (240:1.8);
\path (C) +(0:0.5) coordinate (CA);
\path (C) +(120:0.5) coordinate (CB);
\path (C) +(240:0.5) coordinate (CC);

\draw[be] (AB) -- (AA);
\draw[be] (AC) -- (AA);
\draw[be] (BB) -- (BA);
\draw[be] (BC) -- (BA);
\draw[be] (CB) -- (CA);
\draw[be] (CA) -- (CC);
\draw[be] (BA) to[out=0,in=90] (AA);
\draw[be] (AA) to[out=-90,in=-20] (CC);

\foreach \x in {A,B,C}
  \foreach \y in {A,B,C}
    \fill (\x\y) circle [radius=2pt];

\fill[green!70!black] (CC) circle [radius=2pt,green];

\fill[red] (CB) circle [radius=2pt,fill=red];
\node[above=0 of CB] {\scriptsize{$a_{0}$}};
\node[right=0 of CA] {\scriptsize{$a_{1}$}};
\node[left=0 of CC] {\scriptsize{$a_{2}$}};

\end{tikzpicture}
\caption{In the new tree, $a_{0}$ makes a request for the token at $a_{2}$ in the same virtual node. As with normal Ivy, all nodes will connect back to the node $a_{0}$ who made the original request.}
\end{subfigure}

\begin{subfigure}[t]{0.5\textwidth}
\centering
\begin{tikzpicture}
[be/.style={draw=black,arrows={-Stealth[scale=1.5]}}]
\coordinate (A) at (0:1.8);
\path (A) +(0:0.5) coordinate (AA);
\path (A) +(120:0.5) coordinate (AB);
\path (A) +(240:0.5) coordinate (AC);
\coordinate (B) at (120:1.8);
\path (B) +(0:0.5) coordinate (BA);
\path (B) +(120:0.5) coordinate (BB);
\path (B) +(240:0.5) coordinate (BC);
\coordinate (C) at (240:1.8);
\path (C) +(0:0.5) coordinate (CA);
\path (C) +(120:0.5) coordinate (CB);
\path (C) +(240:0.5) coordinate (CC);

\draw[be] (AB) -- (AA);
\draw[be] (AC) -- (AA);
\draw[be] (BB) -- (BA);
\draw[be] (BC) -- (BA);
\draw[be] (CA) -- (CB);
\draw[be] (CC) -- (CB);
\draw[be] (BA) to[out=0,in=90] (AA);
\draw[be] (AA) to[out=-90,in=-20] (CC);

\foreach \x in {A,B,C}
  \foreach \y in {A,B,C}
    \fill (\x\y) circle [radius=2pt];

\fill[green!70!black] (CB) circle [radius=2pt,green];

\fill[red] (BB) circle [radius=2pt,fill=red];
\node[above=0 of BB] {\scriptsize{$a_{0}$}};
\node[above right=0 and -3pt of BA] {\scriptsize{$a_{1}$}};
\node[right=0 of AA] {\scriptsize{$a_{2}$}};
\node[below left=0 and -3pt of CC] {\scriptsize{$a_{3}$}};
\node[above=0 of CB] {\scriptsize{$a_{4}$}};

\end{tikzpicture}
\caption{The request path has two long edges, invoking the Ivy behavior on the upper level: $a_{3}$ will connect back to $a_{0}$. Then when $a_{4}$ receives the request, the rules disallow it from connecting back to $a_{0}$ as well, since this would lead to multiple edges between virtual nodes. Therefore $a_{4}$ has to connect back to $a_{2}$}
\end{subfigure}
\quad
\begin{subfigure}[t]{0.5\textwidth}
\centering
\begin{tikzpicture}
[be/.style={draw=black,arrows={-Stealth[scale=1.5]}}]
\coordinate (A) at (0:1.8);
\path (A) +(0:0.5) coordinate (AA);
\path (A) +(120:0.5) coordinate (AB);
\path (A) +(240:0.5) coordinate (AC);
\coordinate (B) at (120:1.8);
\path (B) +(0:0.5) coordinate (BA);
\path (B) +(120:0.5) coordinate (BB);
\path (B) +(240:0.5) coordinate (BC);
\coordinate (C) at (240:1.8);
\path (C) +(0:0.5) coordinate (CA);
\path (C) +(120:0.5) coordinate (CB);
\path (C) +(240:0.5) coordinate (CC);

\draw[be] (AB) -- (AA);
\draw[be] (AC) -- (AA);
\draw[be] (BA) -- (BB);
\draw[be] (BC) -- (BA);
\draw[be] (CA) -- (CB);
\draw[be] (CB) -- (CC);
\draw[be] (AA) to[out=90,in=0] (BB);
\draw[be] (CC) to[out=110,in=-110] (BB);

\foreach \x in {A,B,C}
  \foreach \y in {A,B,C}
    \fill (\x\y) circle [radius=2pt];

\fill[green!70!black] (BB) circle [radius=2pt,green];

\end{tikzpicture}
\caption{The resulting tree still only has $2$ long edges due to how the rules were enforced.}
\end{subfigure}
\caption{Example of Recursive Clique algorithm on a recursive clique graph with parameters $b=3$ and $l=2$.}
\label{fig:reclique-alg}
\end{figure}

\section{Fixed Ratio}

\paragraph{Hop-based}

Another simple algorithm is one which chooses the node at a fixed ratio $f\in[0,1]$ between the first and the last one. As an example, with a request path $a_{0},a_{1},a_{2},a_{3},a_{4}$ and a ratio of $f=\frac{1}{2}$, node $a_{5}$ would choose $a_{2}$ as its new parent, since it's half-way between $a_{0}$ and $a_{4}$. We use the earlier node in case of a ratio being in-between two nodes. See Figure~\ref{fig:fpr} for an example.

\begin{algorithmic}
\Function{SelectNewParent}{$\{a_0, a_1,\dots,a_{k-1}\}$}
\State\Return $a_{\lfloor f\cdot(k-1)\rfloor}$
\EndFunction
\end{algorithmic}

\paragraph{Cost-based}

A possible variation of this algorithm doesn't use the fraction on the node counts, but on the edge costs instead. A request path $a_{0},a_{1},a_{2},\dots,a_{k-1}$ gets mapped to costs $c_i$ representing how far the request had to travel to get to node $a_i$. Formally these costs are defined as
\begin{equation}
c_i=\sum_{j=1}^ic(a_{j-1},a_{j})
\end{equation}

The last node whose $c_i$ is lower than or equal to $f\cdot c_{k-1}$ is then selected.

\begin{algorithmic}
\Function{SelectNewParent}{$\{a_{0}, a_{1},\dots,a_{k-1}\}$}
\State\Return $a_{i}$ with the maximum $i$ given the constraint $c_i\leq f\cdot c_{k-1}$ 
\EndFunction
\end{algorithmic}

These algorithms have the following interesting properties:
\begin{itemize}
\item For $f=0$ they are equivalent to Ivy. Similarly with $f=1$ they are equivalent to Arrow.
\item For $f=\frac{1}{m}, m\in\mathbb{N}$, the hop-based version builds a $m$-ary tree on the request path. See Figure~\ref{fig:fpr} for an example of it building a binary tree with $m=2$.
\item If the costs are the same for all edges, the cost-based version is equivalent to the hop-based one.
\end{itemize}

\begin{figure}
\centering
\begin{tikzpicture}
[bn/.style={circle,draw}
,root/.style={bn,thick}
,be/.style={thick,arrows={-Stealth[scale=1]}}
,pe/.style={dashed,draw=blue,arrows={-Stealth[scale=1]}}
,req/.style={bn,red!70!black}
,auto,scale=0.7]
\node[bn] (n0) at (4,0) {$a_{0}$};
\node[bn] (n1) at (1.5,0) {$a_{1}$};
\node[bn] (n2) at (0,2) {$a_{2}$};
\node[bn] (n3) at (1,4) {$a_{3}$};
\node[bn] (n4) at (3,5) {$a_{4}$};
\node[bn] (n5) at (5,4) {$a_{5}$};
\node[bn] (n6) at (6,2) {$a_{6}$};

\draw[pe] (n0) to[out=200,in=-20] (n1);
\draw[pe] (n1) -- node[blue]{\scriptsize{request path}} (n2);
\draw[pe] (n2) -- (n3);
\draw[pe] (n3) -- (n4);
\draw[pe] (n4) -- (n5);
\draw[pe] (n5) -- (n6);

\draw[be] (n1) -- (n0);
\draw[be] (n2) -- (n0);
\draw[be] (n3) -- (n1);
\draw[be] (n4) -- (n1);
\draw[be] (n5) -- (n2);
\draw[be] (n6) -- (n2);
\end{tikzpicture}
\caption{Hop-based Fixed Ratio algorithm with a ratio of $\frac{1}{2}$ and a request path of 7 nodes. Node $a_{6}$ chooses $a_{\lfloor \frac{1}{2}\cdot(6-1) \rfloor}=a_{2}$ as its parent. A binary tree is created as a result.}
\label{fig:fpr}
\end{figure}

\section{Dynamic Star}
\label{dynstar}

To understand the Dynamic Star algorithm, we first look at the results from \cite{Peleg}, which for one is about finding a good tree to use for Arrow. A probability distribution $p_i, i\in V$ for each node making a request is given. Then the chance that a request travels from node $i$ to node $j$ is simply $p_ip_j$. Now let $d_T(i,j)$ be the cost to go from node $i$ to $j$ in some spanning tree $T$, similarly let $d_G(i,j)$ be the cost for the way from $i$ to $j$ by taking the shortest path in graph $G$. Then the expected cost of a single request in a tree and the graph respectively is
\begin{equation}
\begin{split}
c_R(T)\coloneqq & \sum_{i,j}p_ip_jd_T(i,j) \\
c_R(G)\coloneqq & \sum_{i,j}p_ip_jd_G(i,j) \\
\end{split}
\end{equation}

Now let $T_c$ denote a shortest-path tree from center node $c$, meaning a tree for which every node $v$ has a shortest path to $c$, i.e. $d_{T_c}(c,v)=d_G(c,v)$. The paper then shows that the $T_c$ with lowest $c_R(T_c)$ is a 2-approximation of the tree with lowest possible $c_R$:
\begin{equation}
c_R(T_c)\leq 2\cdot c_R(G)
\end{equation}

This means such a $T_c$ is a relatively good tree to use for Arrow, since the cost of the requests isn't much greater than the best possible path they could take. And since in our case we have a complete graph and metric costs, a shortest-path tree from node $c$ can simply be a star, connecting every other node to $c$ directly. With our complete graph weights $c:E\to\mathbb{N}^+$ this then reduces the calculation of $c_R(T_c)$ to
\begin{equation}
\label{dynstarc}
c_R(T_c)=\sum_{i,j}p_ip_j(c(i,c)+c(c,j))
\end{equation}

Now what this Dynamic Star algorithm does is it measures the distribution $p_i$ at runtime and dynamically adjusts the tree according to it, striving for a star. The fact that Arvy is a distributed algorithm however poses a problem, as a node can't obtain a global view of the system to get accurate values for $p_i$, so we'll have to make do with every node $v$ having a local approximation $p_i^v$ of $p_i$.

To implement this the idea is that each node $v$ maintains counts $n_i^v\in\mathbb{N}_0$ of how many times each node $i$ made a request, initialized with all $0$. Because $v$ always knows how many requests it made itself, the value for $n_v^v$ is always accurate, while all others can be an outdated value. To update $n_i^v$ the following rules are used:
\begin{enumerate}
\item When node $v$ makes a request, set $n_v^v\gets n_v^v+1$
\label{rule1}
\item Any message sent from $v$ includes a subset of known counts $\{n_i^v,i\in S\subset V\}$, which when arriving at $u$ will update all received values if they are bigger than the values already known: $n_i^u\gets \max(n_i^u,n_i^v),i\in S$.
\end{enumerate}

Depending on the choice of the subset $S$ of value updates to send, the information propagation of the true values in the network might be slower or faster. Generally the bigger $|S|$, the faster information propagates. Some interesting choices for $S$ are as follows
\begin{itemize}
\item When node $v$ makes a request for the token, it only sends along its own request count with $S=\{v\}$, while all nodes along the request path use this same $S$. Since the only way for the rest of the network to know the accurate request count of the requesting node is for it to send the an updated value, this is one of the most reasonable single-element choices for $S$.
\item With every node forwarding all its values, we get With $S=V$. Propagation of values is as high as possible, but we also have $\mathcal{O}(n)$ message complexity.
\item Message size can be reduced by making $S$ a uniform random selection of $V$, with $|S|\ll|V|$. With many requests happening, information should still propagate rather quickly.
\item An improved version of this is a random weighted selection, preferring $n_i^v$ that have been updated more recently.
\end{itemize}

Once such $n_i^v$ are known, the approximated request probabilities $p_i^v$ are then simply
\begin{equation}
p_i^v=\frac{n_i^v}{\sum_jn_j^v}
\end{equation}

With this we have everything in place for the algorithm to select a new parent out of the available options $\{a_{0},a_{1},\dots,a_{k-1}\}$. We do so by selecting the $a_{i}$ with the lowest cost as the center of the star.

\begin{algorithmic}
\Function{SelectNewParent}{$\{a_{0}, a_{1},\dots,a_{k-1}\}$}
\State\Return Choose $a_{i}$ with lowest $c_R(T_{a_{i}})$
\EndFunction
\end{algorithmic}

Justification is needed for the fact that the node $a_k$ selecting the new parent seems to access costs not imminent to it with the calculation of all $c_R(T_{a_{i}})$, which was given as a restriction in \ref{model}. This can be circumvented by each node $a_{i}$ calculating $c_R(T_{a_{i}})$ locally, which is allowed since in Formula~\ref{dynstarc} only accesses costs next the center node $a_{i}$. Then that value gets sent in the message to all subsequent nodes in the request path, which can use this result for the selection of the minimum. Note however that this is not exactly the same as calculating all $c_R(T_{a_{i}})$ values in the final node in the request path, but the resulting values shouldn't be far off.

Therefore this algorithm needs $\mathcal{O}(n^2)$ time for calculation of $c_R(T_{a_i})$ in each node along the request path, making it very slow to run in comparison to other algorithms. Messages need to send $S$ and as an optimization only the $a_{i}$ with lowest $c_R(T_{a_{i}})$, resulting in $\mathcal{O}(|S|)$ message complexity.

\subsection{Non-converging Dynamic Star}

One slight problem with the Dynamic Star algorithm is that in general it eventually converges to a static tree. This is because as the total number of request grows bigger, changes to $n_i^v$ matter less and less. After $R\in\mathbb{N}$ requests, another request can cause a maximum change to $p_i^v$ of
\begin{equation}
\begin{split}
\left|p_i^v-p_i^{'v}\right| = & \left|\frac{n_i^v}{\sum_jn_j^v}-\frac{n_i^{'v}}{\sum_jn_j^{'v}}\right| = \left|\frac{n_i^v}{R}-\frac{n_i^v+\{0,1\}}{R+1}\right| \\
= & \left|\frac{n_i^vR+n_i^v-n_i^vR+\{0,1\}R}{R(R+1)}\right| = \frac{\left|n_i^v-\{0,1\}R\right|}{R(R+1)} \\
\leq & \frac{R}{R(R+1)}=\frac{1}{R+1}\to_{R\to\infty} 0
\end{split}
\end{equation}

Where $\{0,1\}$ is $1$ or $0$ depending on whether $n_i^v$ was increased or not.

This is a problem because it means the longer a system runs, the less it's adapting to changes in the probability pattern. If after $10^{100}$ requests a request-controlling adversary shows up, it could exploit this practically static tree by constantly bouncing requests between two nodes with the worst connection. If this were to happen initially it wouldn't be a problem as the heuristic could adapt to this by choosing a better star center quickly. This means the algorithm's behavior is dependent on how long it has been running.

The general idea to solve this problem is to give less weight to older requests. This can be done in multiple ways:
\begin{itemize}
\item After every increment of $n_i^v$ by one, all counts get decreased by a constant factor: $\forall i:n_i^v\gets \alpha n_i^v$ with $0<\alpha<1$. If this was a global view this would work well, since after $R$ requests an original contribution to $n_i^{global}$ of $1$ would be reduced to $\alpha^R$, implementing an exponential weight falloff. However since this is not a global view, it won't work as well, because depending on the propagation speed with $|S|$, the $n_i^v$ will be more outdated, resulting in old requests having more weight than they would in a global view.
\item A slight improvement over this can be achieved: For each message sent from $v$, the total amount of requests known $\sum_jn_j^v$ is sent as well. When received by $u$, it's possible for it to calculate the difference of known requests $d\coloneqq\sum_jn_j^v-\sum_jn_j^u$. This means all $n_i^u$ are about that many requests more in the past than $n_i^v$, so an update is done on them to compensate for this: $\forall i:n_i^u\gets\alpha^dn_i^u$. This is better because the multiplication with $\alpha$ doesn't depend on the selection of $|S|$ anymore, resulting in older values being more accurately weighed.
\item With the requirement of nodes being able to measure passing of time, it's possible to improve on this: Every fixed time step, all counts get decreased with the constant factor $\forall i:n_i^v\gets \alpha n_i^v$. This is now a accurate time-based exponential falloff, which is independent of both the propagation speed and the request pattern.
\end{itemize}

In this thesis however we assume that the request pattern is periodic, where the problem described here isn't possible. Therefore this non-converging Dynamic Star algorithm is not further evaluated.

\chapter{Graph Costs, Initial Trees and Request Sequences}

The performance of an Arvy algorithm depends not only on the algorithm itself, but also on the costs of the edges, the initial tree and the request sequence.

\section{Graph Costs}
\label{graph-costs}

Oftentimes an underlying non-complete graph is used to determine the costs of a complete graph by using the shortest route. In non-complete graphs without associated costs, all edges can be given a unit cost of 1.

\paragraph{Clique} This is a cost function based on an underlying graph where all nodes are connected to each other with a unit cost.
\begin{equation}
c(u,v)=
\begin{cases}
0 & \text{if}\;u=v \\
1 & \text{otherwise}
\end{cases}
\end{equation}

\paragraph{Ring} This cost function is based on a ring with unit costs as the underlying graph.
\begin{equation}
c(u,v)\equiv|u-v|\;(\bmod\;n)
\end{equation}

\paragraph{Uniformly Random Hypercube} This function models costs between uniformly random points $p_i:[0,1]^d$ in a $d$-dimensional hypercube, using the direct distance between them as the cost.
\begin{equation}
c(u,v)=\norm{p_u-p_v}
\end{equation}

\paragraph{Erdős-Rényi Random Graph} Generate a non-complete graph $G'$ by connecting every possible edge with probability $p$. Then use that as the underlying graph with each edge having unit length.
\begin{equation}
c(u,v)=\text{length of shortest path from $u$ to $v$ in $G'$}
\end{equation}

\paragraph{Barabási-Albert Random Graph} For an $m\in\mathbb{N}$, start with a graph $G'=(\{1,\dots,m\},\varnothing)$. Then iteratively add additional nodes while connecting it to $m$ existing nodes, with probabilities proportional to their degrees. This generates a scale-free network, where node degrees follow a power law, meaning only few nodes have high degree while many have low degree. The costs are then using this as an underlying graph. With $m=1$ this generates a tree as the underlying graph.

\begin{equation}
c(u,v)=\text{length of shortest path from $u$ to $v$ in $G'$}
\end{equation}

\section{Initial Trees}
\label{initial-trees}

\paragraph{Minimum Spanning Tree}

The minimum spanning tree (MST) is the tree with lowest total edge cost.

\paragraph{Uniformly Random Tree}

A uniformly random tree out of all possible trees. An algorithm for generating such a tree is as follows:

\begin{enumerate}
\item Select a random root node $r\gets_{u.a.r.}V$
\item Define $S=\{r\}$ to be the set of all nodes already included in the tree
\item Select a random node already in the tree $u\gets_{u.a.r.}S$
\label{random-tree-goto}
\item Select a random node not yet in the tree $v\gets_{u.a.r.}V\setminus S$
\item Output edge $(u,v)$ as part of the resulting tree
\item Set $S\gets S\cup\{v\}$
\item Unless $S=V$, go to step \ref{random-tree-goto}
\end{enumerate}

\paragraph{Best Star}

As we learned in Section~\ref{dynstar}, the shortest-path tree with minimum $c_R(T_c)$ for some center node $c$ has at most two times longer paths than direct paths in the graph. By assuming uniformly random requests with $p_i=\frac{1}{n}$ for all nodes $i$, we can use this to generate an initial tree without knowing the request distribution. This also simplifies the calculation for $C_R(T_c)$ to
\begin{equation}
\begin{split}
c_R(T_c)&=\sum_{i,j}\frac{1}{n}\frac{1}{n}(c(i,c)+c(c,j)) \\
&=\frac{1}{n^2}\sum_{i,j}(c(i,c)+c(c,j)) \\
&=\frac{1}{n^2}\sum_{i,j}c(i,c)+\frac{1}{n^2}\sum_{i,j}c(c,j) \\
&=\frac{n}{n^2}\sum_{i}c(i,c)+\frac{n}{n^2}\sum_{j}c(c,j) \\
&=\frac{2}{n}\sum_{i}c(i,c) \\
\end{split}
\end{equation}

This means finding $c_R(T_c)$ for a single $c$ can be done in time $\mathcal{O}(n)$ and for finding the best $c$ with smallest $c_R(T_c)$, $\mathcal{O}(n^2)$ is needed.

\paragraph{Minimum Pair Distance Tree}
\label{tree:mpd}

This initial tree minimizes arguably most important measure for the Arrow algorithm: The average distance between all pairs of nodes. This is important because with uniformly random requests, every pair of nodes has the same chance of occurring subsequently in the request sequence. Consequently the average time to satisfy a request is the average distance between all pairs of nodes. See figure~\ref{fig:minPairDist} for example of such trees. Noticeable is that central nodes are forming and not all short edges are used.

A way to calculate this minimum is by brute force search through all possible trees. However because there are as much as $n^{n-2}$ possible trees~\cite{Borchardt} in a graph of $n$ nodes, this is not practical for more than a couple nodes. A listing of all possible trees can be obtained by the $n^{n-2}$ Prüfer~\cite{Prufer} sequences, for each of which the total pair distance needs to be found which is of order $\mathcal{O}(n^2)$ (with an algorithm similar to the one described in the next paragraph), giving a total complexity of $\mathcal{O}(n^n)$.

\begin{figure}[]
\begin{subfigure}[t]{0.5\textwidth}
\centering
\begin{tikzpicture}[scale=0.5]\footnotesize
\draw[step=1cm,gray,very thin] (0,0) grid (10,10);
\foreach \x/\xtext in { 5/0.5, 10/1}
  \draw[black,xshift=\x cm] (0,.3) -- (0,0)
    node[below] {$\xtext$};
\foreach \y/\ytext in {5/0.5,10/1}
  \draw[black, yshift=\y cm] (.3,0) -- (0,0)
    node[left] {$\ytext$};
\draw[black] (0,0) node[anchor=north east] {O};
\draw[black,thick,->] (0, 0) -- (10.4, 0) node[right] {$x$};
\draw[black,thick,->] (0, 0) -- (0, 10.4) node[above] {$y$};

\coordinate (n0) at (6.2307871625908895,6.847027862484314);
\coordinate (n1) at (1.4908587845403831,9.344058188486937e-2);
\coordinate (n2) at (8.287496589039378,0.8087839284280984);
\coordinate (n3) at (3.662398504604587,0.2833983731779788);
\coordinate (n4) at (0.46715307814554796,7.233804012602063);
\coordinate (n5) at (9.057142532479283,7.467617781163527);
\coordinate (n6) at (7.44580515795162,3.7458151873056766);
\coordinate (n7) at (2.5041109412617235,0.3813699908645396);
\coordinate (n8) at (1.4952402484562444,6.327917885574969);
\coordinate (n9) at (8.966635336689189,0.6898816967621113);
\draw[thick] (n0) -- (n6);
\draw[thick] (n1) -- (n7);
\draw[thick] (n3) -- (n6);
\draw[thick] (n4) -- (n8);
\draw[thick] (n5) -- (n6);
\draw[thick] (n6) -- (n2);
\draw[thick] (n7) -- (n3);
\draw[thick] (n8) -- (n6);
\draw[thick] (n9) -- (n2);

\foreach \n in {n0,n1,n2,n3,n4,n5,n6,n7,n8,n9}
  \fill (\n) circle [radius=3pt];

\end{tikzpicture}
\end{subfigure}
\quad
\begin{subfigure}[t]{0.5\textwidth}
\centering
\begin{tikzpicture}[scale=0.5]\footnotesize
\draw[step=1cm,gray,very thin] (0,0) grid (10,10);
\foreach \x/\xtext in { 5/0.5, 10/1}
  \draw[black,xshift=\x cm] (0,.3) -- (0,0)
    node[below] {$\xtext$};
\foreach \y/\ytext in {5/0.5,10/1}
  \draw[black, yshift=\y cm] (.3,0) -- (0,0)
    node[left] {$\ytext$};
\draw[black] (0,0) node[anchor=north east] {O};
\draw[black,thick,->] (0, 0) -- (10.4, 0) node[right] {$x$};
\draw[black,thick,->] (0, 0) -- (0, 10.4) node[above] {$y$};

\coordinate (n0) at (6.486317401060322,1.7238949217908428);
\coordinate (n1) at (7.356328176128615,6.7307533977302025);
\coordinate (n2) at (3.5627260906386415,7.493578518706636);
\coordinate (n3) at (8.582933996984575,1.1655496682661926);
\coordinate (n4) at (5.35937147771363,8.338368157065618);
\coordinate (n5) at (5.224998480720355,5.848686799435092);
\coordinate (n6) at (4.8023010605002465,3.2954642049633076);
\coordinate (n7) at (7.334875634225844,3.82310788843208);
\coordinate (n8) at (8.606186458999954,3.198710520132577);
\coordinate (n9) at (9.099399064468583,7.859275597076669);
\draw[thick] (n0) -- (n7);
\draw[thick] (n2) -- (n5);
\draw[thick] (n3) -- (n7);
\draw[thick] (n4) -- (n5);
\draw[thick] (n5) -- (n1);
\draw[thick] (n6) -- (n5);
\draw[thick] (n7) -- (n5);
\draw[thick] (n8) -- (n7);
\draw[thick] (n9) -- (n1);
\foreach \n in {n0,n1,n2,n3,n4,n5,n6,n7,n8,n9}
  \fill (\n) circle [radius=3pt];

\end{tikzpicture}
\end{subfigure}
\caption{Examples of 10-node minimum pair distance trees for uniformly random 2-dimensional points in the unit square and euclidean weights between them.}
\label{fig:minPairDist}
\end{figure}

\paragraph{Approximated Mimimum Pair Distance}

Because calculating the actual minimum pair distance tree is impractical, we describe an algorithm that constructs a tree in $\mathcal{O}(n^3)$ time that tries to minimize pair distance as well as possible. Let $c_{G'}(u, v)$ denote the distance of the shortest path from $u$ to $v$ in a tree graph $G'=(V',E')$ of nodes $V'\subset V$ and edges $E'\subset E$. Then let $\mathcal{C}_{pairs}(G')$ denote the total distance between all pairs of nodes in $G'$.
\begin{equation}
\mathcal{C}_{pairs}(G') = \sum_{u,v\in V'}c_{G'}(u,v)
\end{equation}

The algorithm is based on the idea of connecting nodes one-by-one to the tree, in every step choosing the node and a connecting edge that minimizes the total pair distance. We do this efficiently by maintaining a data structure that allows querying the increase of total pair distance for a node and edge in constant time.

Let $V'$ be the set of nodes currently included in the tree. $V'$ gets initialized to $\{r\}$ where $r$ is an arbitrarily chosen node. Our data structure then encompasses an array $p:V'\to\mathbb{R}$, where $p(v)$ stores the total distance from node $v$ to all other nodes $V'\setminus\{v\}$ in the current tree. In addition, a two-dimensional array $s:(V',V')\to\mathbb{R}$ is needed, where $s(u,v)$ stores the shortest distance in the tree between nodes $u$ and $v$.
\begin{equation}
\begin{split}
s(u,v)=&c_{G'}(u,v)\\
p(u)=&\sum_{i\in V'}c_{G'}(i,u) \\
\end{split}
\end{equation}

Now if edge $(u,v)$ with $u\in V'$ and $v\in V\setminus V'$ were to get added to the tree to get $G''=G'\cup(\{v\}, \{(u,v)\})$, the total pair distance increases only by the new pairs from $v$ to all nodes in $V'$. With our data structure we can calculate this increase in constant time:
\begin{equation}
\begin{split}
& \mathcal{C}_{pairs}(G'')-\mathcal{C}_{pairs}(G') \\
=\;&\sum_{i\in V'}c_{G''}(i, v)+\sum_{i\in V'}c_{G''}(v, i)=2\sum_{i\in V'}c_{G''}(i, v) \\
=\;&2\sum_{i\in V'}\left(c_{G'}(i,u)+c(u,v)\right)=2\left(\sum_{i\in V'}c_{G'}(i,u)+|V'|\cdot c(u,v)\right) \\
=\;&2\left(p(u)+|V'|\cdot c(u,v)\right) \\
\end{split}
\end{equation}

In addition, we can update our data structure for the new edge $(u,v)$ in linear time:
\begin{itemize}
\item To set the total distance from $v$ to all other nodes is just the total distance to all other nodes from $u$, plus the new edge once for every node: $p(v)\gets p(u)+|V'|\cdot c(u,v)$
\item For all other nodes, $p$ needs to be increased by the shortest path to $v$, which is just the shortest path to $u$ plus the cost of the edge $(u,v)$: $p(i)\gets p(i)+s(i,u)+c(u,v)$ for all $i\in V'$
\item Also the shortest path from all nodes to $v$ gets initialized to the shortest path to $u$ plus the cost of the edge $(u,v)$: $s(i,v)\gets s(i,u)+c(u,v)$ for all $i\in V'$
\item Similarly the shortest path from $v$ to all nodes: $s(v,i)\gets c(v,u)+s(u,i)$ for all $i\in V'$
\end{itemize}

As a result, our algorithm looks as follows:
\begin{enumerate}
\item Set $V'=\{r\}$ for an arbitrary node $r\in V$ and initialize $p(r)=0$ and $s(r,r)=0$
\item Add the edge $(u,v)$ ($u\in V'$ and $v\in V\setminus V'$) which increases the total pair distance by the minimum to the resulting tree
\label{approxStep}
\item Update $p$ and $s$ according to the new edge
\item Unless $V'=V$, go to step \ref{approxStep}
\end{enumerate}

Because the tree gets one more node in every iteration, the sequence of the number of edges between tree and non-tree nodes is $1n,2(n-1),3(n-2),\dots,(n-1)2,n1$. Every iteration therefore takes $\mathcal{O}(n^2)$ to find the best edge and an additional $\mathcal{O}(n)$ to update the data structure. The total complexity of this algorithm is therefore $\mathcal{O}(n\cdot(n^2+n))=\mathcal{O}(n^3)$.

\TODO{Go into how good this actually is?}
\TODO{Show some examples}

\section{Request Sequences}

This section describes all the 

\paragraph{Uniformly random requests} This request sequence distributes requests uniformly across all nodes.

\paragraph{Adversarial requests} This request sequence looks at the current tree and chooses the node furthest away

\chapter{Implementation Notes}

As part of this thesis, a Haskell library for writing and testing Arvy algorithms was created, accessible at \url{https://github.com/infinisil/arvy}. In this chapter we explain some key design decisions that went into it. Most notably, Haskell's advanced type system allowed guaranteeing correctness of all Arvy heuristics defined with this library. We will look at parts of the file \href{https://github.com/Infinisil/arvy/blob/1bdac2aa8e599372f2b058d26ec9c33fd53d7a72/lib/Arvy/Algorithm.hs}{$\texttt{lib/Arvy/Algorithm.hs}$} where the core of this library is defined. Needless to say, this chapter is more technical than the others and advanced Haskell knowledge is required to fully understand it.

\section{Request Path abstraction}

An Arvy algorithm is only correct if and only if nodes can only set their new parent to nodes that were previously visited on the request path. With each node being referenced as an integer, this would be a problem, because a node $a_{k}$ could simply set $parent(a_{k})=0$ even though it might be the case that $0\notin\{a_{0},\dots,a_{k-1}\}$ which would make the algorithm incorrect. So instead of representing nodes as integers, we represent them as some arbitrary type with certain capabilities that reflect it being part of the request path.

One such trivial capability we'll need is that once we have some node index, we can forward that value via messages to other nodes. This makes sense since some node $a_{k}$ can't know about $\{a_{0},\dots,a_{k-1}\}$ unless these nodes somehow forwarded their own values along with the messages. In Haskell, this forwarding can be represented as a typeclass on two types $i_a$ and $i_b$, stating that if you have a value of $i_a$ you can get a value of $i_b$. We can implement this typeclass trivially for all types that are equal, since forwarding will then be the identity.

\begin{minted}{haskell}
-- | A class for node indices that can be forwarded between nodes
class Forwardable ia ib where
  forward :: ia -> ib

-- | All equivalent types can be trivially forwarded
instance Forwardable i i where
  forward = id
\end{minted}

Now we need to somehow encode a request path in a type. In such a request path the important constraint is that messages are only sent in one direction, meaning node indices can only be forwarded in one direction as well. Looking at it from a single node in the request path, we can forward from its predecessor to the current node and from the current node to its successor. Encoding this as a typeclass leaves us with the following for representing a node (index).

\begin{minted}{haskell}
class ( Forwardable (Pred i) i
      , Forwardable i (Succ i)
      ) => NodeIndex i where
  -- The type representing the previous node
  type Pred i :: *
  -- The type representing the successor node
  type Succ i :: *
\end{minted}

Now of course defining different types for every node along the request path is impossible, this is only used to prove correctness. So for the implementation, we can use integers to represent all nodes.

\begin{minted}{haskell}
type Node = Int

instance NodeIndex Node where
  -- Predecessors and successors in the request path are
  -- all nodes represented by integers as well
  type Pred Node = Node
  type Succ Node = Node
\end{minted}

\section{Arvy Heuristic abstraction}

The most important part of defining an Arvy algorithm is the heuristic it uses to decide the new node parent. We extend this to the notion of a behavior, which includes not only the new parents, but also how it gets decided what a request message looks like, how the initial message is generated, and what the final node receiving the message does. In fact each node on the request path should be allowed to run arbitrary code when they do the processing.

For representing computations we use the relatively new effect library \href{https://hackage.haskell.org/package/polysemy}{Polysemy}. In this library a computation is described with the type $\texttt{Sem r a}$ where $\texttt{r}$ represents the possible effects it can have and $\texttt{a}$ being the result of the computation. As an example, a computation that's allow to use randomness and returns a boolean is described by the type $\texttt{Sem '[Random] Bool}$. With this said, the main type for defining an Arvy algorithm is defined as follows.

\begin{minted}{haskell}
{- |
An Arvy heuristic for a dynamic algorithm.
- @i@ is the node index type
- @msg :: * -> *@ is the type of request messages passed between nodes,
  parametrized by the node index type for allowing messages to forward
  node indices
- @r@ is the effects the algorithm runs in, which can include effects
  parametrized by @i@ in order to allow effects dependent on indices
-}
data ArvyBehavior i msg r = ArvyBehavior
  { arvyMakeRequest :: i -> Succ i -> Sem r (msg i)
  -- ^ What message the node making the request should send to its parent
  , arvyForwardRequest :: msg (Pred i) -> i -> Succ i -> Sem r (Pred i, msg i)
  -- ^ What parent an intermediate node should select when receiving
  -- a message, and what message to forward to its parent
  , arvyReceiveRequest :: msg (Pred i) -> i -> Sem r (Pred i)
  -- ^ What parent the last node should select when receiving a message
  }
\end{minted}

This succinct-yet-complicated type includes a lot of information. When a node makes a request for the token, $\texttt{arvyMakeRequest}$ is called which generates a message to be sent to the nodes parent. Then when a message arrives at a node, depending on whether it has the token or not, the functions $\texttt{arvyReceiveRequest}$ or $\texttt{arvyForwardRequest}$ are called respectively, the latter of which then needs to generate another message for forwarding. Function $\texttt{arvyForwardRequest}$ is the most interesting one since it has to handle both receiving and sending a message, so we'll look at its type signature a bit closer.

For one it says that an intermediate node receives a message $\texttt{msg (Pred i)}$ that can include node indices of nodes that were predecessors to the current node. It then needs to return a value of $\texttt{Pred i}$, which represents the new parent for the node. With how node indices are defined in the previous section, the only possible values to return are ones received from the message. This means to select a new parent, nodes have to decode the message which will contain node indices given by past nodes, then return one of them.

In addition, the function receives values $\texttt{i}$ and $\texttt{Succ i}$ representing the current node index and the current nodes parent respectively. In addition to the new parent node to select, the function also needs to return a $\texttt{msg i}$ representing the message to be forwarded. The $\texttt{msg}$ type being parametrized by $\texttt{i}$ enforces that this message can only contain node indices to previous nodes (since all previous node indices $\texttt{Pred i}$ can be forwarded to $\texttt{i}$) or the current node (which is of type $\texttt{i}$ already). $\texttt{Succ i}$ however can't be sent in the message because there's no way to convert it to type $\texttt{i}$.

Functions $\texttt{arvyMakeRequest}$ and $\texttt{arvyReceiveRequest}$ are special cases of $\texttt{arvyForwardRequest}$. The former of which is special in that a node making a request does not receive any message and it doesn't need to select a new parent since it becomes the root. The latter of which is special in that the final root node receiving the request doesn't have a parent and doesn't need to forward a message.

\chapter{Results}

In this chapter we first look at the evaluation results from simulating different scenarios for Arvy. In each of the scenarios we have the following possible variations:

We then run different Arvy algorithms on such scenarios, which we can compare against each other in a single plot. In addition we specify the initial tree used for these algorithms, which can be one of the ones specified in Section~\ref{initial-trees}. The measures we will be looking at include mainly the ones described in Section~\ref{model-perf}. Note that we will be using a moving average of the 100 last requests for $\mathcal{C}_{time}$ to get much smoother plots.

\pgfplotsset{
  every axis legend/.append style={
    at={(0.5,1.03)},
    anchor=south
  },
  width=0.9\textwidth,
  height=0.5\textwidth,
  legend columns = 3,
  every axis/.append style = {
    xmode = log,
    xmin = 1,
    grid,
    colormap name = colormap/jet,
    legend style = { font = \scriptsize },
    xlabel = Number of requests,
    no markers,
  },
  every axis plot post/.append style = {
    thick,
    x={x},
    line join=round,
  },
}


\section{Comparison between initial trees for Arrow}

For this evaluation we run Arrow on different initial trees to see how well they perform on uniformly random requests. We use only 10 nodes such that we can compute the minimum pair distance tree in reasonable time. For graph costs we use uniformly random points from a 2-dimensional hypercube. First we look at how the average time to satisfy requests changes over time.

\begin{tikzpicture}
\centering
\begin{axis}[
  ylabel = Average request time $\mathcal{C}_{time}$,
  xmax = 100000,
  cycle list = { [samples of colormap=5] },
]
\pgfplotstableread{data/trees/time.dat}{\data}
\addplot table [y={arrow-random}] {\data};
\addlegendentry {Random}
\addplot table [y={arrow-mst}] {\data};
\addlegendentry {MST}
\addplot table [y={arrow-shortpairs}] {\data};
\addlegendentry {Approx. pair minimizer}
\addplot table [y={arrow-star}] {\data};
\addlegendentry {Best star}
\addplot table [y={arrow-shortestpairs}] {\data};
\addlegendentry {Pair minimizer}
\end{axis}
\end{tikzpicture}

Not surprisingly, the tree minimizing total pair distance outperforms every other initial tree. Due to its running time of $\mathcal{O}(n^n)$ however it won't be very practical. Interestingly the next best one is the best star tree with only a running time of $\mathcal{O}(n^2)$, followed closely by the approximated total pair distance minimizer with runtime $\mathcal{O}(n^3)$. The MST comes next with a slightly worse time. Understandably, a completely random tree is not a good tree to use for fast average request time.

Conclusively, for all future evaluations of Arrow we will use the Best Star tree.

Now let's also look at the number of hops for each of them.

\begin{tikzpicture}
\centering
\begin{axis}[
  ylabel = Average request hops $\mathcal{C}_{hops}$,
  xmax = 100000,
  cycle list = { [samples of colormap=5] },
]
\pgfplotstableread{data/trees/hops.dat}{\data}
\addplot table [y={arrow-random}] {\data};
\addlegendentry {Random}
\addplot table [y={arrow-mst}] {\data};
\addlegendentry {MST}
\addplot table [y={arrow-shortpairs}] {\data};
\addlegendentry {Approx. pair minimizer}
\addplot table [y={arrow-star}] {\data};
\addlegendentry {Best star}
\addplot table [y={arrow-shortestpairs}] {\data};
\addlegendentry {Pair minimizer}
\end{axis}
\end{tikzpicture}

Here as expected the star tree has the lowest hop counts out of all trees. This is lower than $2$ because nodes can make requests when they have the token already, giving a hop count of $0$, and the center node always has a hop count $\leq 1$. Interestingly the uniformly random tree tree has a decently low hop count.

Note that while this is only the result for a single random seed with only 10 nodes, the results are the same for other random seeds and node counts. Also since in this case the tree never changes, it would be possible to calculate accurate expectations for these measures.

\section{Tree Convergence}

In this section we look at the behavior of algorithms in regards to the tree they end up with after many uniformly random requests. To get a feeling for the tree we look at the tree's average edge cost. We use only $100$ nodes since the Dynamic Star algorithm is quite slow to run. For edge costs we use uniformly random points in a 2-dimensional hypercube.

In addition to looking at the total tree edge cost for algorithms, we also plot some fixed baselines for known trees. This includes one for the MST, one for the Best Star tree, and one for a completely random tree.

All algorithms start with an initially completely random tree.

\begin{tikzpicture}
\centering
\begin{axis}[
  ylabel = Average tree edge cost,
  xmax = 100000,
  cycle list = { [samples of colormap=8] },
]
\pgfplotstableread{data/converging/treeWeight.dat}{\data}
\addplot table [y={arrow-star}] {\data};
\addlegendentry {Tree: Best Star}
\addplot table [y={arrow-mst}] {\data};
\addlegendentry {Tree: MST}
\addplot table [y={arrow-random}] {\data};
\addlegendentry {Tree: Random}
\addplot table [y={ivy-random}] {\data};
\addlegendentry {Alg: Ivy}
\addplot table [y={random-random}] {\data};
\addlegendentry {Alg: Random algorithm}
\addplot table [y={localMinPairs-random}] {\data};
\addlegendentry {Alg: Local Pair Minimizer}
\addplot table [y={minWeight-random}] {\data};
\addlegendentry {Alg: Edge Cost Minimizer}
\addplot table [y={dynamicStar-random}] {\data};
\addlegendentry {Alg: Dynamic Star}
\end{axis}
\end{tikzpicture}

What we can see from this plot:
\begin{itemize}
\item The Dynamic Star algorithm converges to the Best Star tree, indicated by their lines merging into one. The fact that the line does not jitter indicates a converged tree. This makes sense since they both optimize for a minimal $c_R(T_c)$ with a uniform request distribution. Only with very bad luck of a bunch of requests to the same nodes in a row it is possible for it to stray from a Best Star tree, getting more improbable the longer it runs.
\item The Edge Cost Minimizer algorithm as expected converges to the MST, which is already explained in Section~\ref{alg:ecm}. Here we can directly see how this algorithm only ever decreases average edge cost, indicated by its line monotonically decreasing until it reaches the lowest possible state of the MST.
\item The Ivy algorithm seems to continuously have a tree with high average edge cost, in the same range as a completely random tree. This makes more or less sense since Ivy does not look at edge costs at all when selecting new node parents. If we were to include the Fixed Ratio algorithm in this plot it would show the same result for the same reason.
\item A completely random algorithm similarly doesn't produce any better tree.
\item Interestingly, the Local Pair Minimizer algorithm seems to go towards a rather low average edge cost with no known baseline in that region.
\end{itemize}

From this we conclude that the eventual behavior of the Dynamic Star and Edge Cost Minimizer algorithms can be achieved by simply using Arrow on the Best Star tree and the MST respectively. Therefore we don't further look at these algorithms.

\section{Fixed Ratio algorithms}

Here we look at the Fixed Ratio class of algorithms, both hop-based and weight-based ones, with different values for the ratio $f$, including $f=0$ for Ivy and $f=1$ for Arrow.

\begin{tabular}{ r | l }
  Node count & 1000 \\ \hline
  Graph weights & Uniformly random points in 2-dim unit hypercube \\ \hline
  Algorithms & Arrow with Best Star and hop-based \\
  & Fixed Ratio with $f=\frac{i}{8}$ and random initial tree \\ \hline
  Requests & $10^6$ uniformly random \\
\end{tabular}
\TODO{This for every plot}

\begin{tikzpicture}
\centering
\begin{axis}[
  ylabel = Average request distance,
  xmax = 1000000,
  cycle list = { [samples of colormap=10] },
  legend columns = 5,
]
\pgfplotstableread{data/ratio/time.dat}{\data}
\addplot table [y={arrow-star}] {\data};
\addlegendentry {Best Arrow}
\addplot table [y={inbetween-0-1-random}] {\data};
\addlegendentry {$f=\frac{0}{8}$ (Ivy)}
\addplot table [y={inbetween-1-8-random}] {\data};
\addlegendentry {$f=\frac{1}{8}$}
\addplot table [y={inbetween-1-4-random}] {\data};
\addlegendentry {$f=\frac{2}{8}$}
\addplot table [y={inbetween-3-8-random}] {\data};
\addlegendentry {$f=\frac{3}{8}$}
\addplot table [y={inbetween-1-2-random}] {\data};
\addlegendentry {$f=\frac{4}{8}$}
\addplot table [y={inbetween-5-8-random}] {\data};
\addlegendentry {$f=\frac{5}{8}$}
\addplot table [y={inbetween-3-4-random}] {\data};
\addlegendentry {$f=\frac{6}{8}$}
\addplot table [y={inbetween-7-8-random}] {\data};
\addlegendentry {$f=\frac{7}{8}$}
\addplot table [y={inbetween-1-1-random}] {\data};
\addlegendentry {$f=\frac{8}{8}$ (Arrow)}
\end{axis}
\end{tikzpicture}

From this we can see that generally, the lower the ratio, the shorter average request costs. This is somewhat explainable that with a lower ratio shortcuts are taken, which are advantageous since we started with a completely random tree. However, for some reason ratios around $f=\frac{6}{8}$ seem to converge to some average request count lower than all others. So mysteriously it seems that $f=\frac{6}{8}=\frac{3}{4}$ with hop-based Fixed Ratio is a not too bad algorithm to use for this scenario. \TODO{explanation?}

We now also look at the weight-based Fixed Ratio algorithm:

\begin{tikzpicture}
\centering
\begin{axis}[
  ylabel = Average request distance,
  xmax = 100000,
  cycle list = { [samples of colormap=10] },
  legend columns = 5,
]
\pgfplotstableread{data/ratio2/time.dat}{\data}
\addplot table [y={arrow-star}] {\data};
\addlegendentry {Best Arrow}
\addplot table [y={inbetween2-0.0-random}] {\data};
\addlegendentry {$f=\frac{0}{8}$ (Ivy)}
\addplot table [y={inbetween2-0.125-random}] {\data};
\addlegendentry {$f=\frac{1}{8}$}
\addplot table [y={inbetween2-0.25-random}] {\data};
\addlegendentry {$f=\frac{2}{8}$}
\addplot table [y={inbetween2-0.375-random}] {\data};
\addlegendentry {$f=\frac{3}{8}$}
\addplot table [y={inbetween2-0.5-random}] {\data};
\addlegendentry {$f=\frac{4}{8}$}
\addplot table [y={inbetween2-0.625-random}] {\data};
\addlegendentry {$f=\frac{5}{8}$}
\addplot table [y={inbetween2-0.75-random}] {\data};
\addlegendentry {$f=\frac{6}{8}$}
\addplot table [y={inbetween2-0.875-random}] {\data};
\addlegendentry {$f=\frac{7}{8}$}
\addplot table [y={inbetween2-1.0-random}] {\data};
\addlegendentry {$f=\frac{8}{8}$ (Arrow)}
\end{axis}
\end{tikzpicture}

With an initial look, the anomalies from the previous plot are not present anymore. However they now appear in the opposite direction: The ratios around $f=\frac{3}{4}$ are all \textit{above} the highest ratio, giving a performance ordering from best to worst of $\frac{0}{8}, \frac{1}{8}, \frac{2}{8}, \frac{3}{8}, \frac{8}{8}, \frac{4}{8}, \frac{5}{8}, \frac{6}{8}, \frac{7}{8}$. This is very peculiar and we can't explain the reason for this.

\section{Comparison between algorithms}

We now look at all algorithms still relevant, ignoring ones that eventually converge to a static tree.

\begin{tikzpicture}
\centering
\begin{axis}[
  ylabel = Average request distance,
  xmax = 100000,
  cycle list = { [samples of colormap=4] },
  legend columns = 4,
]
\pgfplotstableread{data/algs/time.dat}{\data}
\addplot table [y={arrow-star}] {\data};
\addlegendentry {Best Arrow}
\addplot table [y={ivy-random}] {\data};
\addlegendentry {Ivy}
\addplot table [y={localMinPairs-random}] {\data};
\addlegendentry {Local Pair Minimizer}
\addplot table [y={inbetween-3-4-random}] {\data};
\addlegendentry {Hop-based Fixed Ratio with $f=\frac{3}{4}$}
\end{axis}
\end{tikzpicture}

\begin{tikzpicture}
\centering
\begin{axis}[
  ylabel = Hops,
  xmax = 100000,
  cycle list = { [samples of colormap=4] },
  legend columns = 4,
]
\pgfplotstableread{data/algs/hops.dat}{\data}
\addplot table [y={arrow-star}] {\data};
\addlegendentry {Best Arrow}
\addplot table [y={ivy-random}] {\data};
\addlegendentry {Ivy}
\addplot table [y={localMinPairs-random}] {\data};
\addlegendentry {Local Pair Minimizer}
\addplot table [y={inbetween-3-4-random}] {\data};
\addlegendentry {Hop-based Fixed Ratio with $f=\frac{3}{4}$}
\end{axis}
\end{tikzpicture}

\section{Adversarial requests}

\begin{tikzpicture}
\centering
\begin{groupplot}[
  xmax = 100000,
  cycle list = { [samples of colormap=5] },
  legend columns = 2,
  group style = {
    group size = 1 by 2,
    xlabels at = edge bottom,
    ylabels at = edge left,
  },
]
\nextgroupplot[ylabel=Average request distance]
\pgfplotstableread{data/adversary/time.dat}{\data}
\addplot table [y={arrow-star}] {\data};
\addlegendentry {Best Arrow}
\addplot table [y={ivy-random}] {\data};
\addlegendentry {Ivy}
\addplot table [y={localMinPairs-random}] {\data};
\addlegendentry {Local Pair Minimizer}
\addplot table [y={inbetween-3-4-random}] {\data};
\addlegendentry {Hop-based Fixed Ratio with $f=\frac{3}{4}$}
\addplot table [y={dynamicStar-random}] {\data};
\addlegendentry {Dynamic Star}

\nextgroupplot[ylabel=Hops]
\pgfplotstableread{data/adversary/hops.dat}{\data}
\addplot table [y={arrow-star}] {\data};
\addplot table [y={ivy-random}] {\data};
\addplot table [y={localMinPairs-random}] {\data};
\addplot table [y={inbetween-3-4-random}] {\data};
\addplot table [y={dynamicStar-random}] {\data};
\end{groupplot}
\end{tikzpicture}

\subsection{Ivy in small cliques}
\label{result:clique}

The fact that Arrow on the Best Star seems to have been the best algorithm gives the impression that we don't need any algorithms that change the tree. However let's look at Ivy in a clique of a low number of nodes

\begin{tikzpicture}
\begin{groupplot}[
  ylabel = Average request distance,
  xmax = 100000,
  cycle list = { [samples of colormap=2] },
  small,
  group style = {
    group size = 2 by 2,
    xlabels at = edge bottom,
    ylabels at = edge left,
    vertical sep = 1.5cm,
  },
]
\nextgroupplot[title={$3$ nodes},legend to name={CommonLegend1}]
\pgfplotstableread{data/low3/time.dat}{\3}
\addplot table [y={ivy-shortestpairs}] {\3};
\addlegendentry{Ivy}
\addplot table [y={arrow-shortestpairs}] {\3};
\addlegendentry{Arrow}

\nextgroupplot[title={$4$ nodes}]
\pgfplotstableread{data/low4/time.dat}{\4}
\addplot table [y={ivy-shortestpairs}] {\4};
\addplot table [y={arrow-shortestpairs}] {\4};

\nextgroupplot[title={$5$ nodes}]
\pgfplotstableread{data/low5/time.dat}{\5}
\addplot table [y={ivy-shortestpairs}] {\5};
\addplot table [y={arrow-shortestpairs}] {\5};

\nextgroupplot[title={$6$ nodes}]
\pgfplotstableread{data/low6/time.dat}{\6}
\addplot table [y={ivy-shortestpairs}] {\6};
\addplot table [y={arrow-shortestpairs}] {\6};

\end{groupplot}
\path[text=black] (group c1r1.north east) -- node[above=0.5cm]{\ref{CommonLegend1}} (group c2r1.north west);
\end{tikzpicture}

\subsection{Recursive clique vs others}

\begin{tikzpicture}
\begin{groupplot}[
  ylabel = Average request distance,
  xmax = 1000000,
  cycle list = { [samples of colormap=5] },
  small,
  group style = {
    group size = 2 by 1,
    xlabels at = edge bottom,
    ylabels at = edge left,
  },
]
\pgfplotstableread{data/reclique/time.dat}{\data}
\nextgroupplot[legend to name={CommonLegend}]
\addplot table [y={reclique}] {\data};
\addlegendentry {Reclique}
\addplot table [y={arrow-star}] {\data};
\addlegendentry {Best Arrow}
\addplot table [y={ivy-random}] {\data};
\addlegendentry {Ivy}
\addplot table [y={localMinPairs-random}] {\data};
\addlegendentry {Local Pair Minimizer}
\addplot table [y={inbetween-3-4-random}] {\data};
\addlegendentry {Hop-based Fixed Ratio with $f=\frac{3}{4}$}

\nextgroupplot[xmin=1000]
\addplot table [y={reclique}] {\data};
\addplot table [y={arrow-star}] {\data};
\end{groupplot}
\path[text=black] (group c1r1.north east) -- node[above]{\ref{CommonLegend}} (group c2r1.north west);
\end{tikzpicture}


\chapter{Summary}

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
